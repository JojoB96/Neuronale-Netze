\documentclass{beamer}

\usepackage[utf8]{inputenc}


%Information to be included in the title page:
\title{Mancala}
\author{Viktor Kosin und Johanna Beier}
\date{04.09.2020}



\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Introduction}
\end{frame}

\begin{frame}
\frametitle{Mancala}
\begin{itemize}
\item ancient two player game
\end{itemize}
\includegraphics[scale=0.15]{board}
\end{frame}

\begin{frame}
\begin{itemize}
\item \textbf{as vector:} $[6,6,6,6,6,6, | 6,6,6,6,6,6, |0,0]$
\item\textbf{Goal:} catch more then half of the beans (37) 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Mancala Rules}
\begin{itemize}
\item collect all beans of a hole and drop one in each clockwise following hole
\item catch all beans of the last hole, if it contains $6$, $4$ or $2$ beans
\item going backwards: collect beans from all following holes with $6$, $4$ or $2$ beans, if there are no other holes in between
\item game ends if either one player has no more beans or one player catches at least $37$ beans
\item total sum of beans: catched beans $+$ beans on own side
\end{itemize}
\end{frame}

\begin{frame}
 \frametitle{MDP}
 \begin{itemize}
 \item Mancala can be represented as a Marcov Decision Process (MDP)
 \item set of states S, set of actions per state A, action a $\in$ A
 \item How does the Mancala agent learn to choose the best action?
 \end{itemize}
 \end{frame}

 \begin{frame}
 \frametitle{Reinforcement Learning}
 \begin{itemize}
 \item \textbf{Idea:} reward or punish some action
 \item \textbf{Goal of agent:} maximize total reward
 \item \textbf{here:} Small reward for catching beans, bigger reward for winning the game
 \item use Q-Learning
 \end{itemize}
 \end{frame}
 
  \begin{frame}
  \frametitle{Q-learning}
 \begin{itemize}
 \item small state space: Q-table
  \item replace Q-table by Q-function 
  \begin{align}
  Q(s,a) \leftarrow Q(s,a)+\alpha [r+\gamma \max_{a'} Q(s',a') -Q(s,a)]
  \end{align}
 \item agents often need to learn actions that do not lead immediately to a reward
 \item allow a small amount of random actions (exploration rate)
 \end{itemize}
 \end{frame}
 
 \begin{frame}
 \frametitle{Netz}
 bild netz, dass wir verwenden
 \end{frame}
  \begin{frame}
 \frametitle{Netz}
\begin{itemize}
\item \textbf{activation function:} Sigmoidfunction
\item learningrate for the update-weights-function
\end{itemize}
 \end{frame}
 
 \begin{frame}
 \frametitle{Backpropagtion}
 \begin{itemize}
 \item[\textbf{1. Step}] Generate training data: for a given input set an expected output (e.g. with Q-function)
 \item[\textbf{2. Step}] Calculate for the input $a^{x,1}$:
 \begin{itemize}
 \item activation $a^{x,l}$ of layer $l=2,3,...,L$ by
 $$a^{x,l} = \sigma(w^l a^{x,l-1} + b^l)$$
 \item Output error $\delta^{x,L}$
 \item Backpropagate error to each layer: $\delta^{x,l}$
 \end{itemize}
 \item[3. Step] Use error of each layer to update weights and biases 
 \end{itemize}
 \end{frame}
 
 \begin{frame}
 \frametitle{play}
 \begin{itemize}
\item choose action by feeding the current board to the net and take the argmax of the output
\item get spielfeld and reward after action(self.spielfeld, action)
\item append board and reward to trainingslists
\item change player (turn board)
\end{itemize}
 \end{frame}
 
 
 \begin{frame}
 \frametitle{Training}
 \begin{itemize}
\item let two agents play against each other and save pairs of actions and rewards
\item update for each boardstate and action the underlying Q-function
\item save each board state and dedicated Q-values as training data
\item feedforward a boardstate to the net
\item loss $=$ output $-$ Q-values
\end{itemize}
 \end{frame}
 
\begin{frame}
\frametitle{Ideas for finding errors}
\begin{itemize}
\item reduce rules $\rightarrow$ get very simple game
\item reduce board size from 12 to 6/4 holes with 4/2 beans
\item print q-values, feedforward and compare
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{simple rules}
\begin{itemize}
\item game wins or tied after $6$ trainingsiterations
\item q-function and feedforward look good
\end{itemize}
\end{frame}


\begin{frame}
\begin{itemize}
\frametitle{simple board}
\item
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Quellen}
%bild: https://1tr7g949k6uv1s2cx31wn2yt-wpengine.netdna-ssl.com/wp-content/uploads/2017/02/handmade-%mancala-board-makemesomethingspecial.com_-2.jpg
\end{frame}
\end{document}