\documentclass{beamer}

\usepackage[utf8]{inputenc}


%Information to be included in the title page:
\title{Mancala}
\author{Viktor Kosin und Johanna Beier}
\date{04.09.2020}



\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Introduction}
\end{frame}

\begin{frame}
\frametitle{Mancala}
\begin{itemize}
\item ancient two player game
\end{itemize}
%\includegraphics[scale=0.15]{board}
\end{frame}

\begin{frame}
\begin{itemize}
\item \textbf{as vector:} $[6,6,6,6,6,6, | 6,6,6,6,6,6, |0,0]$
\item\textbf{Goal:} catch more then half of the beans (37) 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Mancala Rules}
\begin{itemize}
\item collect all beans of a hole and drop one in each clockwise following hole
\item catch all beans of the last hole, if it contains $6$, $4$ or $2$ beans
\item going backwards: collect beans from all following holes with $6$, $4$ or $2$ beans, if there are no other holes in between
\item game ends if either one player has no more beans or one player catches at least $37$ beans
\item total sum of beans: catched beans $+$ beans on own side
\end{itemize}
\end{frame}

\begin{frame}
 \frametitle{MDP}
 \begin{itemize}
 \item Mancala can be represented as a Marcov Decision Process (MDP)
 \item set of states S, set of actions per state A, action a $\in$ A
 \item How does the Mancala agent learn to choose the best action?
 \end{itemize}
 \end{frame}

 \begin{frame}
 \frametitle{Reinforcement Learning}
 \begin{itemize}
 \item \textbf{Idea:} reward or punish some action
 \item \textbf{Goal of agent:} maximize total reward
 \item \textbf{here:} Small reward for catching beans, bigger reward for winning the game
 \item use Q-Learning
 \end{itemize}
 \end{frame}
 
  \begin{frame}
  \frametitle{Q-learning}
 \begin{itemize}
 \item small state space: Q-table
  \item replace Q-table by Q-function 
  \begin{align}
  Q(s,a) \leftarrow Q(s,a)+\alpha [r+\gamma \max_{a'} Q(s',a') -Q(s,a)]
  \end{align}
 \item agents often need to learn actions that do not lead immediately to a reward
 \item allow a small amount of random actions (exploration rate)
 \end{itemize}
 \end{frame}
 
 \begin{frame}
 \frametitle{Netz}
 bild netz, dass wir verwenden
 \end{frame}
  \begin{frame}
 \frametitle{Netz}
\begin{itemize}
\item \textbf{activation function:} Sigmoidfunction
\item learningrate for the update-weights-function
\end{itemize}
 \end{frame}
 
 \begin{frame}
 \frametitle{play}
 \begin{itemize}
\item choose action by feeding the current board to the net and take the argmax of the output
\item get spielfeld and reward after action(self.spielfeld, action)
\item append board and reward to trainingslists
\item change player (turn board)
\end{itemize}
 \end{frame}
 
\begin{frame}
 \frametitle{Rewards and Discount}
 \begin{itemize}
 	\item choosing the right rewards is key for the Q-Function to work
 	\begin{itemize}
 		\item too high rewards lead to a growing q-values (maybe higher than the activation function would allow)
 		\item too low rewards lead to q-values close to zero
 		\item get 0.05 as reward for every caught item and 1.0 for a winning action
 	\end{itemize}
 \item 
 \end{itemize}
\end{frame}
 
 
 \begin{frame}
 \frametitle{Training}
 \begin{itemize}
\item let two agents play against each other and save pairs of actions and rewards
\item update for each boardstate and action the underlying Q-function
\item save each board state and dedicated Q-values as training data
\item feedforward a boardstate to the net
\item loss $=$ output $-$ Q-values
\end{itemize}
 \end{frame}
 
 \begin{frame}
 \frametitle{Backpropagtion}
 \begin{itemize}
 \item[\textbf{1. Step}] Generate training data: for a given input set an expected output (e.g. with Q-function)
 \item[\textbf{2. Step}] Calculate for the input $a^{x,1}$:
 \begin{itemize}
 \item activation $a^{x,l}$ of layer $l=2,3,...,L$ by
 $$a^{x,l} = \sigma(z^{x,l}),\quad z^{x,l} = w^l a^{x,l-1} + b^l$$
 \item Output error $\delta^{x,L} = \nabla_a C_x \odot \sigma'(z^{x,L})$
 \item Backpropagate error to each layer: 
 $$\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1}) \odot \sigma' (z^{x,L})$$
 \end{itemize}
 \item[\textbf{3. Step}] Use error of each layer to update weights and biases 
 \end{itemize}
 \end{frame}
 
\begin{frame}
\frametitle{Ideas for finding errors}
\begin{itemize}
\item reduce rules $\rightarrow$ get very simple game
\item reduce board size from 12 to 6/4 holes with 4/2 beans
\item print q-values, feedforward and compare
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{simple rules}
\begin{itemize}
\item game wins or tied after $6$ trainingsiterations
\item q-function and feedforward look good
\end{itemize}
\end{frame}


\begin{frame}
\begin{itemize}
\frametitle{simple board}
\item 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Results}
\includegraphics[scale=0.65]{gewinnrate.jpg}
\begin{itemize}
\item simple q-learning tends to stagnate very early
\item the q-values of multiple possible actions get close to 1
\item a 'large' learning rate doesn't always improve the rate 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Problems}
\begin{itemize}
\item during the game the number of items caught per turn varies strongly and lead to very high or very low rewards / q-values
\begin{itemize}
\item[$\rightarrow$] the error of hidden layers is poorly approximated due to $\sigma'$
\end{itemize}
\item the q-function is overestimating actions
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Possible Solutions}
\begin{itemize}
\item choose a different / more variable reward strategy
\item add some kind of tree search algorithm or estimate reward for multiple actions with another agent
\item use more than one neuronal network to reduce overestimation like 'Double Q-Learning' or 'Asynchronous Advantage Actor-Critic' 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Quellen}
%bild: https://1tr7g949k6uv1s2cx31wn2yt-wpengine.netdna-ssl.com/wp-content/uploads/2017/02/handmade-%mancala-board-makemesomethingspecial.com_-2.jpg
\end{frame}
\end{document}